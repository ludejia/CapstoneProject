---
output:
  html_document: rmarkdown::github_document
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, comment=NA, message=FALSE,warning = FALSE)
```

# CKME 136 Capstone Project 
# Melbourne House Sales Price Predictive Analysis
#### For best display, please visit project github page https://github.com/ludejia/CapstoneProject
#### Analysis and code are in the R mark down file ‘Code.RMD’

#### Dejia Lu
#### Ryerson University 501005537

## Introduction

Since 2010, Melburne housing market was experiencing a housing bubble and some expert said it might burst soon.  However, there was a cooling period in 2018. The contributor of the housing market data set was considering buying an apartment. He was seeking data experts to help him draw some insights on the data to assist his decision making.

In Capstone Project, my goal is to explore and understand the relation between the final sold price and different variable factors, perform Predictive Analytics using various machine learning algorithms, compare the performances and differences among these models and find the best model for property price prediction. The procedures I will be following are exploratory analysis, date cleaning, feature engineering, model building and model evaluation.

## Literature Review

After reviewing books, website, journals and video, I found some methodologies and techniques are especially helpful in data visualization and preparation, feature selection & modelling building, and model performance evaluation.

EDA (exploratory data cleaning) is a very important step when conducting initial analyse, De Jonge & Van Der Loo’s book (2013) detail various data preprocessing techniques including missing value handling, data conversion and data manipulation. They also give examples in R environment. They suggested various way to impute missing value such as hot-deck imputation and knn imputation. The book ‘Hands-on exploratory data analysis with R’(Datar & Garg, 2019) not only demonstrates practical data manipulation process using different R packages but also dedicated a big portion in data visualizing utilizing ggplot2 package which makes the data much more easily to be understood. For example, in scatter plot and density plot, colors are used as a new dimension to differentiate categories.

The dataset provides GPS location information for each house. It would be especially helpful to visualize the houses on the map and to color these points by different properties to understand the geographic impact on the houses. The book ggmap: Spatial visualization with ggplot2(Kahle & Wickham, 2013) showed the step by step approach to import map from google map and plot those points in a meaningful way.

In terms of feature selection and modeling building, An introduction to statistical learning: With applications in R (James et al. 2013) has a comprehensive coverage on all the popular algorithms and their mathematical explanations. For each algorithm, it has multiple lab exercise in R environment dedicated to it so you can apply the theory to real dataset. In the section of linear model selection, it provided multiple ways to select the best features of the dataset. I learned Ridge Regression, Lasso Regression, Random Forest and Support Vector Machine and their application in this book.  The elements of statistical learning: Data mining, inference, and prediction (Hastie et al., 2009) gives an in-depth math and statistical concepts on modelling and model evaluations. Due to its missing of application in R environment, I used this book as a supplement reading to the previous modelling book for the concepts that I am not so clear about.


## Dataset, data manipulation, feature engineering and selection

The dataset I am using was posted on Kaggle and scraped by the contributor Pino(2018) from an Australian property website (Domain.com.au) for the period from 2016 to 2018. The data set contains the propterty information of has 34857 observations. Each observation has 21 attributes.
Price is the attribute I am building model to predict. Below provides the summary for the data and visualizaiton of missing values.

```{r}
library(ggplot2)
library(ggmap)
library(visdat)
library(gridExtra)
library(MASS)
theme_set(theme_bw())
options(scipen=999)
house=read.csv('data/Melbourne_housing_FULL.csv')
missing=sapply(house,function(x){sum(is.na(x))})
missing=as.data.frame(missing,row.names = NULL)
missing$var=colnames(house)
missing$missing=missing$missing*100/nrow(house)
missing=missing[order(missing$missing),]
ggplot(missing)+geom_bar(aes(x=reorder(var, missing),y=missing),
                         stat = "identity", fill='grey',color='black')+
        coord_flip()+
        labs(title='attributes and their missing value percentage', y='Missing Value Percentage',x='')
vis_miss(house)
```

#### *Attributes explanation and selection*


#### Lattitude & Longtitude 

"Lattitude" & "Longtitude" are the GPS location data. 23% of the data are missing. Below we can visualize the location of the properties on google map and see which area has the high density of the property.


```{r}
register_google(key = "AIzaSyCyjfy52N-yvo76okcOhh4sc1wSqSMi8oI", write = TRUE)
map=get_map(location='Melbourne',zoom = 10,maptype = "terrain",color = "bw",)
ggmap(map)+geom_point(data = house, aes(x = house$Longtitude, y = house$Lattitude), 
                 color ='red', size = 0.05,alpha=0.15)


```

#### Price & PricePerSquareMeters

"Price" is the sold price in Australian dollar for the property. I replace the unit of price to million Australian dollars. I created a feature "PriceSQM" to represent price per square meter as it's also the common measure of property value and easier to understand its interaction with the features. I remove the data points having missing value in price and building area, and remove the outliers on PriceSQM using boxplot. After the cleaning, total number of observations reduces to 10395. Below shows the missing value after cleaning and price per square meter distribution. 

I removed the outliers by discarding the data with price per square meter less than 2000 and over 15000 based on reasonable price range in Melbourne.


```{r}
#remove missing values
house=house[!is.na(house$Price),]
house=house[house$Price!=0,]
house=house[!is.na(house$BuildingArea),]
house=house[house$BuildingArea!=0,]
house$PriceSQM=house$Price/house$BuildingArea
house=house[house$PriceSQM<15000 & house$PriceSQM>2000,]
house$Price=house$Price/1000000


vis_miss(house)
ggplot(house)+geom_histogram(aes(x=house$Price),
                 binwidth = 0.1, fill='grey', col='black')+
        labs(x='House sold price (million)')
```


#### BuildingArea, Landsize, Building Area Ratio

"Landsize" is the size of the land the property occupies. "BuildingArea" is the floor square meters of the house, town house or unit. For unit or townhouse, landsize could be very big but the building area might be small as the land are shared by many units or townhouses. 

Data cleaning:
For the house type, I remove the data having building areas less than 40 square meters.

For landsize, there are missing values, zero landsize, unreasonable large landsize and landsizes that are smaller than building area. I impute these values by the median of different house types.

I replace landsize with Building Area Ratio ( Building Area / Landsize), because for unit and townhouse, multiple properties share one land, and Building Area Ratio would be relative small. Building Area ratio reflects the interaction between landize and building type.



```{r}
house=house[!(house$BuildingArea<40 & house$Type=='h'),]
house[house$Landsize<house$BuildingArea& ! is.na(house$Landsize),]$Landsize=NA
house[is.na(house$Landsize) & house$Type=='u',]$Landsize=median(house[house$Type=='u',]$Landsize,na.rm = T)
house[is.na(house$Landsize) & house$Type=='h',]$Landsize=median(house[house$Type=='h',]$Landsize,na.rm = T)
house[is.na(house$Landsize) & house$Type=='t',]$Landsize=median(house[house$Type=='t',]$Landsize,na.rm = T)

#create building area ratio
house$BuildingAreaRatio=house$BuildingArea/house$Landsize

plot1=ggplot(house)+geom_histogram(aes(x=house$Landsize),binwidth = 20,
                             fill='grey',col='black')+
        coord_cartesian(xlim=c(0,900))+
        labs(x='landsize')
plot2=ggplot(house)+geom_histogram(aes(x=house$BuildingArea),binwidth = 20,
                             fill='grey',col='black')+
        coord_cartesian(xlim=c(0,700))+
        labs(x='building area')

grid.arrange(plot1, plot2, ncol=2)

```




#### Suburb, Postcode, Regionname, CouncilArea

"Suburb", "Postcode", "Regionname", "CouncilArea" divide Melbourne into different sections. Below we can find Suburb has the finest divisions (351) whereas regionname only has 9 divisions.

I will only use CouncilArea and region name as the rest have too many categories.

```{r}
Suburb=nrow(table(house$Suburb))
RegionName=nrow(table(house$Regionname))
CouncilArea=nrow(table(house$CouncilArea))
PostalCode=nrow(table(house$Postcode))
Division=data.frame(a=c('Suburb',"RegionName",'CouncilArea','PostalCode'),
                    b=c(Suburb,RegionName,CouncilArea,PostalCode))
ggplot(Division)+geom_bar(aes(x=reorder(a, b),y=b),
                         stat = "identity", fill='grey',col='black', width=0.6)+
        coord_flip()+
        labs(y='Number of Cateories',x='')

```

This map shows the boundaries of Suburb, Postal Code, Council and Region.

![](./map/map.png)

Below is the price per square meter distribution for different regions. Quite obvious, southern metropolitan is more expensive. The red the points on the map are these properties in Southern Metropolitan.

I use council area and Region name as geigraphycal variable and discard the rest including GPS info.

```{r}
ggplot(house)+geom_density(aes(PriceSQM,fill=house$Regionname),alpha=0.4)+
        labs(x='price per square meter')

southern=house[house$Regionname=='Southern Metropolitan',]
ggmap(map)+geom_point(data = southern, aes(x = Longtitude, y = Lattitude), 
                 color ='red', size = 0.05,alpha=0.15)



```

#### Propertycount

"Propertycount" is number of properties that exist in the suburb where the property is located. We can see most properties are in a suburb which has less than 50 properties in its suburb. I converted it from categorial to numeric.

```{r}
df_pcount=data.frame(table(house$Propertycount,useNA = 'always'))
ggplot(df_pcount)+geom_histogram(aes(x=df_pcount$Freq),binwidth = 30, 
                                 fill='grey', col='black')+
        labs(x='number of properties in its suburb')
house$Propertycount=as.numeric(house$Propertycount)

```

#### address

Each property has its own address. It would not be included in the model as we already have Regionname, CouncilArea as geographical features.

#### Type

There are three types of houses.

"House" reprents house,cottage,villa, semi,terrace.

"Unit" reprents apartment, condo, duplex.

"Townhouse" repsents townhouse only.

Below we could see the total number of each type and sold price distribution.

```{r}
house$Type=as.character(house$Type)
house[house$Type=='u',]$Type='unit'
house[house$Type=='h',]$Type='house'
house[house$Type=='t',]$Type='townhouse'
house$Type=as.factor(house$Type)
ggplot(house)+geom_bar(aes(x=house$Type),fill='grey',width = 0.3,col='black')+
        labs(x='house type')
ggplot(house)+geom_density(aes(house$Price,fill=house$Type),alpha=0.4)+
        labs(x='price')

```


#### Rooms, bedroom2

"Rooms" and "bedroom2" are very similar, which is the number of bedrooms in the property. "Rooms" has no missing value and "bedroom2" has 23% percent missing value and is scraped by the contributor from different sources. "bedroom2" would be discarded in the model.

```{r}
ggplot(house)+geom_bar(aes(x=house$Rooms), fill='grey',col='black')+
        coord_cartesian(xlim=c(0,16),)+ 
        scale_x_continuous(breaks=seq(0, 16, 1))+
        labs(x='Number of Room')


```        

#### bathroom

"bathroom" is the number of bathroom. Below we can find "bathroom" and "rooms" are correlated.

```{r}
pb1=ggplot(house)+geom_bar(aes(x=house$Bathroom), fill='grey',col='black')+
        coord_cartesian(xlim=c(0,16),)+ 
        scale_x_continuous(breaks=seq(0, 16, 1))+
        labs(x='Number of bathroom')

pb2=ggplot(house)+geom_point(aes(x=house$Rooms,y=house$Bathroom))+
        labs(x='number of room', y='number of bathroom')

grid.arrange(pb1, pb2, ncol=2)
print(cor.test(house$Rooms,house$Bathroom))
```

#### car

"car" is the number of parking spots each property has. The bar chart shows the distribution. Car has 1.67% missing values which are imputed using median.

```{r}
house[is.na(house$Car),]$Car=median(house$Car,na.rm=T)
ggplot(house)+geom_histogram(aes(x=house$Car),
                        col='black',fill='grey')+
        coord_cartesian(xlim=c(0,10),)+
        scale_x_continuous(breaks=seq(0, 10, 1))+
        labs(x='number of parking spot',
             y='number of properties')

```

#### Distance

"Distance" is the distance in km from the property to the center business district.

```{r}
house$Distance=as.character(house$Distance)
house$Distance=as.numeric(house$Distance)
ggplot(house)+geom_histogram(aes(x=house$Distance), 
                             fill='grey',col='black',binwidth = 2)+
        labs(x='Distance to CBD in km', y='number of properties')
```

#### yearbuilt

The year the property is built. I group years to diffent categories to visualize the data. The categories are "<1900", "1900~1949", "1950~1979", "1980~1999",'2000~2010'. 'yearbuilt' has some missing values. For yearbuilt, I will use numeric format.

I replace one data point built before 1880 with median value.

```{r}
house[is.na(house$YearBuilt),]$YearBuilt=median(house$YearBuilt,na.rm=T)
df_year=as.data.frame(table(house$YearBuilt))
colnames(df_year)=c('year','count')
df_year$year=as.numeric(as.character(df_year$year))
df_year$year50 <- cut(df_year$year, c(0, 1899,1949,1979,1999,2050),
                      include.lowest = T,
                      right = TRUE)
levels(df_year$year50) <- c("<1900", "1900~1949", "1950~1979", "1980~1999",'>2000')
ggplot(df_year)+geom_bar(aes(x=df_year$year50, y=df_year$count),
                         stat = "identity",
                         fill='black', width = 0.5)+
        labs(title="Histogram for year built", x='year',
             y='number of properties')+
        coord_flip()
house[house$YearBuilt<1800,]$YearBuilt=1970

```

#### date & AVGmonth_price

"date" is the date the property was sold. The plot shows the average price per squar meter for all the property sold per month. We could find the the average month price fluctuate greatly which is a big factor affacts the price. Instead of using the date, we would use average price per square meter for all the property sold that specific month as a variable which reflects seanonal impact of market fluctuation and economical influency such as demand and supply.There are only 16 data points for the first month so the average price would can not reflect the true mean very well. So these data points are removed.


```{r}
library(lubridate)
df_num=data.frame(table(house$Date))
colnames(df_num)=c('Date','count')
df_num$Date=dmy(df_num$Date)
df_num$Date=floor_date(df_num$Date, unit = "month")
df_num=aggregate(df_num$count~df_num$Date,df_num,sum)
df_num=df_num[c(-1,-2),]
pd1= ggplot(df_num)+geom_line(aes(x=df_num$`df_num$Date`,y=df_num$`df_num$count`),
                                   color="grey")+
        geom_point(aes(x=df_num$`df_num$Date`,y=df_num$`df_num$count`),
                                                          color="black")+
        labs(title='Property sold per month',x='date', y='number of properties sold ')


df_avghouseprice=aggregate(house$PriceSQM~house$Date,house,mean)
colnames(df_avghouseprice)=c('Date','average sold price')
df_avghouseprice$Date=dmy(df_avghouseprice$Date)
df_avghouseprice$Date=floor_date(df_avghouseprice$Date, unit = "month")
df_avghouseprice=aggregate(df_avghouseprice$`average sold price`~df_avghouseprice$Date,
                           df_avghouseprice,mean)
df_avghouseprice=df_avghouseprice[-1,]
colnames(df_avghouseprice)=c('Date','average sold price')
pd2= ggplot(df_avghouseprice)+geom_line(aes(x=df_avghouseprice$Date,y=df_avghouseprice$`average sold price`),
                                   color="grey")+
        geom_point(aes(x=df_avghouseprice$Date,y=df_avghouseprice$`average sold price`),
                  color="black")+
        labs(title='Average sold price per square meter',x='date', y='average sold price per square meter')
grid.arrange(pd1, pd2, ncol=2)

house$Date=dmy(house$Date)
house$Date=floor_date(house$Date, unit = "month")
house=house[!(house$Date=='2016-02-01'),]
house$AVGprice=df_avghouseprice[match(house$Date,df_avghouseprice$Date),2]

```


#### method

"method" reflects how the house was sold. Below are the meanings of different method and the distribution chart.

PI - property passed in; 

S - sold not disclosed; 

SA - sold after auction; 

SP - property sold prior; 

VB - vendor bid

```{r}
ggplot(house)+geom_bar(aes(x=house$Method), fill='grey', col='black',
              width = 0.5)+
        theme(axis.text.x = element_text(angle = 30,vjust = 0.5))+
        labs(x='method')

```

#### SellerG
SellerG is Seller informaion and will not be included.

#### Final dataset to build the model with no missing values
```{r}
#remove the variables that I don't use
house=house[,-which(names(house)=='Date')]
house=house[,-which(names(house)=='SellerG')]
house=house[,-which(names(house)=='Suburb')]
house=house[,-which(names(house)=='Address')]
house=house[,-which(names(house)=='Postcode')]
house=house[,-which(names(house)=='Bedroom2')]
house=house[,-which(names(house)=='Lattitude')]
house=house[,-which(names(house)=='Longtitude')]
house=house[,-which(names(house)=='PriceSQM')]


#clean unused factor
house$Method=as.character(house$Method)
house$Method=as.factor(house$Method)
house$CouncilArea=as.character(house$CouncilArea)
house$CouncilArea=as.factor(house$CouncilArea)
house$Regionname=as.character(house$Regionname)
house$Regionname=as.factor(house$Regionname)

print(str(house))
vis_miss(house)
```

## Approach

![](./map/approach.png)

#### Step 1: Exploratory Data Analysis

Explore and visualize each attribute and its relation to the prediction variable; Perform data cleaning and feature engineering to find or create the best the relevant features to feed into the model;

#### Step 2: Model Building & Tuning

Build models using various algorithms; Fine tuning parameters to achieve best performance for each algorithm; Adjust features if needed;

#### Step 3: Model Evaluation

Perform cross-validation to compare performance across different models and decide on the final model.

### linear Modelling

Below is the correlation graph of all the numeric variables and we can find that number of rooms are hight correlated with number of bathroom. Price per square meter is highly correlated with YearBuilt and Distance from CBD.


```{r}
library(ggcorrplot)
house_numeric=house
house_numeric=house_numeric[,-which(names(house_numeric)=='Type')]
house_numeric=house_numeric[,-which(names(house_numeric)=='Method')]
house_numeric=house_numeric[,-which(names(house_numeric)=='CouncilArea')]
house_numeric=house_numeric[,-which(names(house_numeric)=='Regionname')]


corr <- cor(house_numeric)
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlation of numeric variables", 
           ggtheme=theme_bw)


```




### linear model
Below we implemented forward, backward, forward stepwise and backward stepwise for feature selection, which all select the same feature including Rooms, Type, Method, Distance, Bathroom, Car, YearBuilt, CouncilArea, Regionname, BuildingAreaRatio, AVGprice. Propertycount is not selected for all the methods.


```{r, eval=F, echo=T}
null=lm(Price~1,data=house)
full=lm(Price~.,data=house)
stepAIC(null, scope=list(lower=null, upper=full), direction= "forward", trace=TRUE)
stepAIC(full, direction= "backward", trace=TRUE)
stepAIC(null, scope=list(lower=null, upper=full), direction= "both", trace=TRUE)
stepAIC(full, direction= "both", trace=TRUE)
```

Below are the summary of the model.Adjusted R-squared is 0.7548 for this model. I plot the the prediction, residual of this model.
```{r}
model_linear=lm(formula = Price ~ Rooms + Type + Method + Distance + Bathroom + 
    Car + BuildingArea + YearBuilt + CouncilArea + Regionname + 
    BuildingAreaRatio + AVGprice, data = house)
summary(model_linear)
residual=data.frame(residual=model_linear$residuals, prediction=model_linear$fitted.values)
pred=data.frame(price=model_linear$model$Price, prediction=model_linear$fitted.values)

ggplot(residual)+geom_histogram(aes(x=residual),
                             binwidth = 0.1, fill='grey', col='black')+
        coord_cartesian(xlim=c(-1.5,1.5))+
        scale_x_continuous(breaks=seq(-1.5,1.5,0.5))+
        labs(title = 'histogram of residuals (unit: million dollars)')

ggplot(pred)+geom_point(aes(y=prediction,x=price), color='blue', alpha=0.10)+
        coord_cartesian(xlim=c(0,3), ylim=c(0,3))+
        labs(title='price vs predection (unit: million)', x='price (million)')

ggplot(residual)+geom_point(aes(x=prediction,y=residual), color='purple', alpha=0.10)+
        labs(title='prediction vs residual (unit: million)',
             x='prediction ', y='residual')+
        coord_cartesian(xlim=c(0,2), ylim=c(-2,2))

```


From the above graph, we find that the residual shows high level of residual heteroscedasticity. So I decide to transform the price to log price to minimize the heteroscedasticity. After the transformation, we could find from the plot below that log(price) becomes normally distributed.

```{r}
house$log_Price=log(house$Price)
trp1=ggplot(house)+geom_histogram(aes(x=house$Price),
                 binwidth = 0.1, fill='grey', col='black')+
        labs(x='House sold price (million)')
trp2=ggplot(house)+geom_histogram(aes(x=house$log_Price),
                             binwidth = 0.1, fill='grey', col='black')+
        labs(x='log House sold price')
grid.arrange(trp1, trp2, ncol=2)
house=house[,-which(names(house)=='Price')]

```

Now I build the model again based using same feature selection methods like before which all produce the same result and achieve higher adjust R square. This time 'BuildingAreaRatio' is not selected by all the feature selection methods. I discard this feature in our dataset. Our final model for linear model is below and adjusted R squared of 0.8206. From the residual plot we can find that heteroscedasticity is not obvious.

```{r}
model_linear=lm(formula = log_Price ~ Rooms + Type + Method + Distance + Bathroom + 
    Car + BuildingArea + YearBuilt + CouncilArea + Regionname + 
    Propertycount + AVGprice+Landsize, data = house)
summary(model_linear)
residual=data.frame(residual=model_linear$residuals, prediction=model_linear$fitted.values)
pred=data.frame(price=model_linear$model$log_Price, prediction=model_linear$fitted.values)

ggplot(residual)+geom_histogram(aes(x=residual),
                                binwidth = 0.1, fill='grey', col='black')+
        coord_cartesian(xlim=c(-1.5,1.5))+
        scale_x_continuous(breaks=seq(-1.5,1.5,0.5))+
        labs(title = 'histogram of residuals (unit: million dollars)')

ggplot(pred)+geom_point(aes(y=prediction,x=price), color='blue', alpha=0.10)+
        coord_cartesian(xlim=c(-2,2), ylim=c(-2,2))+
        labs( x='price(log)', y='prediction(log)')

ggplot(residual)+geom_point(aes(x=prediction,y=residual), color='purple', alpha=0.10)+
        labs(x='prediction(log) ', y='residual(log)')+
        coord_cartesian(xlim=c(-2,2), ylim=c(-0.7,0.7))
house=house[,-which(names(house)=='BuildingAreaRatio')]
```



### Algorithm evaluation and selection




## Reference

De Jonge, E., & Van Der Loo, M. (2013). An introduction to data cleaning with R. Heerlen: Statistics Netherlands.

Datar, R., & Garg, H. (2019). Hands-on exploratory data analysis with R: Become an expert in exploratory data analysis using R packages (1st ed.). UK: Packt Publishing.

Kahle, D., & Wickham, H. (2013). ggmap: Spatial visualization with ggplot2. The R Journal, 5(1), 144. doi:10.32614/RJ-2013-014

James, G., Hastie, T., Tibshirani, R., & Witten, D. (2013). An introduction to statistical learning: With applications in R Springer New York.

Hastie, T., Tibshirani, R., Friedman, J. H., & SpringerLink (Online service). (2009;2013;2001;). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). New York, NY: Springer. doi:10.1007/978-0-387-21606-5

Pino, T. P. (2018, October 14). Melbourne Housing Market. Retrieved May 7, 2020, from https://www.kaggle.com/anthonypino/melbourne-housing-marke

Melbourne Suburb Map. (n.d.). Retrieved June 7, 2020, from http://melbournesuburbsmap.com

LocalCouncilMaps. (n.d.). Retrieved June 7, 2020, from https://enrol.vec.vic.gov.au/ElectoralBoundaries/LocalCouncilMaps.html

Melbourne Regions. (n.d.). Retrieved June 7, 2020, from https://www.education.vic.gov.au/about/department/structure/Pages/regions.aspx



