---
output:
  html_document: rmarkdown::github_document
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, comment=NA, message=FALSE,warning = FALSE,cache=TRUE)
```

# CKME 136 Capstone Project 
# Melbourne House Sales Price Predictive Analysis
#### For best display, please visit project github page https://github.com/ludejia/CapstoneProject
#### Analysis and code are in the R mark down file ‘Code.RMD’

#### Dejia Lu
#### Ryerson University 501005537

## 1. Introduction

Since 2010, Melburne housing market was experiencing a housing bubble and some expert said it might burst soon.  However, there was a cooling period in 2018. In Kaggle competition forum, the contributor of the Melburne Housing Market dataset was considering buying an apartment. He was seeking data experts to help him draw some insights on the data to assist his decision making.

My Capstone Project is based on this dataset, I performed exploratory data analysis to understand the data, cleaned and transformed data for feature engineering and selection, applied 5 machine learning algorithms to compare the pros & cons, and accuracies of these models and found the best algorithm for property price prediction. 


## 2. Literature Review

After reviewing books, website, journals and videos, I found some methodologies and techniques that are especially helpful in data exploration, visualization, feature engineering, modeling and model evaluation.

EDA (exploratory data cleaning) is a very important step when conducting initial analyse, De Jonge & Van Der Loo’s book (2013) detail various data preprocessing techniques including missing value handling, data conversion and data manipulation. They also give examples in R environment. They suggested various way to impute missing value such as hot-deck imputation and KNN imputation. The book ‘Hands-on exploratory data analysis with R’ (Datar & Garg, 2019) not only demonstrates practical data manipulation processes using different R packages but also dedicates a big portion in data visualizing utilizing ggplot2 package which makes the data much more easily to be understood. For example, in scatter plot and density plot, colors are used as a new dimension to differentiate categories.

The dataset provides GPS location information for each house. It would be especially helpful to visualize the houses on the map and to color these points by different properties to understand the geographic impact on the houses. The book ggmap: Spatial visualization with ggplot2 (Kahle & Wickham, 2013) showed the step by step approach to import map from google map and plot those points in a meaningful way.

In terms of feature selection and model building, An introduction to statistical learning: With applications in R (James et al. 2013) has a comprehensive coverage on all the popular algorithms and their mathematical explanations. For each algorithm, it has multiple lab exercise in R environment dedicated to it so you can apply the theory to real dataset. In the section of linear model selection, it provides multiple ways to select the best features of the dataset. I learned Ridge Regression, Lasso Regression, Random Forest and Support Vector Machine and their application in this book.  The elements of statistical learning: Data mining, inference, and prediction (Hastie et al., 2009) gives an in-depth math and statistical concepts on modelling and model evaluations. Due to its missing of application in R environment, I used this book as a supplement reading to the previous modelling book for the concepts that I am not so clear about.

## 3. Approach

![](./map/approach.png)

### Step 1: Exploratory Data Analysis

Explore and visualize each attribute to understand its characteristics

### Step 2: Data Tranformation

### Step 2: Model Building & Tuning

Build models using various algorithms; Fine tuning parameters to achieve best performance for each algorithm; Adjust features if needed;

#### Step 3: Model Evaluation

Perform cross-validation to compare performance across different models and decide on the final model.


## 4. Data

### 4.1 Data Overview 


The dataset I am using was posted on Kaggle and scraped by the contributor Pino (2018) from an Australian property website (Domain.com.au) for the period from 2016 to 2018. The data set contains the property information of 34857 observations. Each observation has 21 attributes. Here is the meaning of all the attributes.

![](./map/attribute.png)
Price is the attribute I am building models to predict. The following chart provides the summary for the attribute names and visualization of missing values percentage we can find that building area has the highest missing value percentage.

```{r}
library(ggplot2)
library(ggmap)
library(visdat)
library(gridExtra)
library(MASS)
library(lubridate)
library(ggcorrplot)
library(caret)
library(randomForest)
library(caret)
library(glmnet)
library(mlbench)
library(psych)

theme_set(theme_bw())
options(scipen=999)
house=read.csv('data/Melbourne_housing_FULL.csv')
missing=sapply(house,function(x){sum(is.na(x))})
missing=as.data.frame(missing,row.names = NULL)
missing$var=colnames(house)
missing$missing=missing$missing*100/nrow(house)
missing=missing[order(missing$missing),]
ggplot(missing)+geom_bar(aes(x=reorder(var, missing),y=missing),
                         stat = "identity", fill='grey',color='black')+
        coord_flip()+
        labs(title='attributes and their missing value percentage', y='Missing Value Percentage',x='')
vis_miss(house)
```

Below we can visualize the location of the properties Melbourne using GPS location in. 23% of the data don’t have GPS information. Each red spot represents a property.

```{r}
register_google(key = "AIzaSyCyjfy52N-yvo76okcOhh4sc1wSqSMi8oI", write = TRUE)
map=get_map(location='Melbourne',zoom = 10,maptype = "terrain",color = "bw",)
ggmap(map)+geom_point(data = house, aes(x = house$Longtitude, y = house$Lattitude), 
                 color ='red', size = 0.05,alpha=0.15)

```


### 4.2 Data Transformation

#### 4.2.1 Price & PriceSQM & Log_Price

"Price" is the sold price in Australian dollar for the property. I replace the unit of price to million Australian dollars. I created a feature "PriceSQM" to represent price per square meter as it's also the common measure of property value and easier to understand its interaction with the features. I removed the data points having missing value in price and building area, and removed the outliers on PriceSQM using boxplot. After the cleaning, total number of observations reduces to 10057. Below shows the missing value after cleaning and price per square meter distribution. 

I removed the outliers by discarding the data with price per square meter less than 2000 and over 15000 based on reasonable price range in Melbourne.

After the data cleaning, as we can see below, missing values are significantly reduced

```{r}
#remove missing values
house=house[!is.na(house$Price),]
house=house[house$Price!=0,]
house=house[!is.na(house$BuildingArea),]
house=house[house$BuildingArea!=0,]
house$PriceSQM=house$Price/house$BuildingArea
house=house[house$PriceSQM<15000 & house$PriceSQM>2000,]
house$Price=house$Price/1000000
vis_miss(house)
```
The chart below is the distribution of the price, we can see the distribution of the price. 

```{r}
ggplot(house)+geom_histogram(aes(x=house$Price),
                 binwidth = 0.1, fill='grey', col='black')+
        labs(x='House sold price (million)')
```
Because it is not normally distributed and will negatively impact the modeling, I applied log transformation to transform price to log(price). Below shows log(price) is normally distributed.

```{r}
house$log_Price=log(house$Price)
ggplot(house)+geom_histogram(aes(x=house$log_Price), fill='grey', col='black')+
        labs(x='House sold log_Price')
```


#### 4.2.2 BuildingArea, Landsize, BuildingAreaRatio

Landsize is the size of the land the property occupies. BuildingArea is the floor square meters of the house, town house or unit. For unit or townhouse, Land size could be very big but the building area might be small as the land are shared by many units or townhouses. 

For house type, I removed the data having building areas less than 40 square meters.

For Landsize, there are missing values, zero, unreasonable large land size and land size that are smaller than building area. I replaced these erroneous values with the median land size of different house types.

I created a new feature BuildingAreaRatio which is BuildingArea / Landsize. Because for unit and townhouse, multiple properties share one common land space, building Area ratio reflects the interaction between land size and building type.

Below charts show the distributions of land size and building area.


```{r}
house=house[!(house$BuildingArea<40 & house$Type=='h'),]
house[house$Landsize<house$BuildingArea& ! is.na(house$Landsize),]$Landsize=NA
house[is.na(house$Landsize) & house$Type=='u',]$Landsize=median(house[house$Type=='u',]$Landsize,na.rm = T)
house[is.na(house$Landsize) & house$Type=='h',]$Landsize=median(house[house$Type=='h',]$Landsize,na.rm = T)
house[is.na(house$Landsize) & house$Type=='t',]$Landsize=median(house[house$Type=='t',]$Landsize,na.rm = T)
#create building area ratio
house$BuildingAreaRatio=house$BuildingArea/house$Landsize
plot1=ggplot(house)+geom_histogram(aes(x=house$Landsize),binwidth = 20,
                             fill='grey',col='black')+
        coord_cartesian(xlim=c(0,900))+
        labs(x='landsize')
plot2=ggplot(house)+geom_histogram(aes(x=house$BuildingArea),binwidth = 20,
                             fill='grey',col='black')+
        coord_cartesian(xlim=c(0,700))+
        labs(x='building area')
grid.arrange(plot1, plot2, ncol=2)

```




#### 4.2.3 Suburb, Postcode, Regionname, CouncilArea

Suburb, Postcode, Regionname, CouncilArea divide Melbourne into different sections.
![](./map/map.png)

Below we can find Suburb has the finest divisions (351) whereas Regionname only has 9 divisions. In my model, I will only use CouncilArea, Regionname as geographical area feature.
```{r}
house$CouncilArea=as.character(house$CouncilArea)
house$CouncilArea=as.factor(house$CouncilArea)
house$Regionname=as.character(house$Regionname)
house$Regionname=as.factor(house$Regionname)
```

```{r}
Suburb=nrow(table(house$Suburb))
RegionName=nrow(table(house$Regionname))
CouncilArea=nrow(table(house$CouncilArea))
PostalCode=nrow(table(house$Postcode))
Division=data.frame(a=c('Suburb',"RegionName",'CouncilArea','PostalCode'),
                    b=c(Suburb,RegionName,CouncilArea,PostalCode))
ggplot(Division)+geom_bar(aes(x=reorder(a, b),y=b),
                         stat = "identity", fill='grey',col='black', width=0.6)+
        coord_flip()+
        labs(y='Number of Cateories',x='')

```


Below is the price per square meter distribution for different regions. Quite obvious, southern metropolitan is more expensive. The red the points on the map are these properties in Southern Metropolitan.

```{r}
ggplot(house)+geom_density(aes(PriceSQM,fill=house$Regionname),alpha=0.4)+
        labs(x='price per square meter (AUS)',y='')+
        theme(legend.title = element_blank())
```

#### 4.2.4 Propertycount

Propertycount is the total number of properties in the suburb where the property is located. We most suburbs have less than 50 properties. I converted the data format from categorial to numeric.

```{r}
df_pcount=data.frame(table(house$Propertycount,useNA = 'always'))
ggplot(df_pcount)+geom_histogram(aes(x=df_pcount$Freq),binwidth = 30, 
                                 fill='grey', col='black')+
        labs(x='number of properties in its suburb')
house$Propertycount=as.numeric(house$Propertycount)

```

#### 4.2.5 Type

There are three types of houses."House" reprents house,cottage,villa, semi,terrace."Unit" reprents apartment, condo, duplex.
"Townhouse" repsents townhouse only. I converted its data class to factor.

Below sold price distribution for each type.

```{r}
house$Type=as.character(house$Type)
house[house$Type=='u',]$Type='unit'
house[house$Type=='h',]$Type='house'
house[house$Type=='t',]$Type='townhouse'
house$Type=as.factor(house$Type)
```

```{r}
ggplot(house)+geom_bar(aes(x=house$Type),fill='grey',width = 0.3,col='black')+
        labs(x='house type')
ggplot(house)+geom_density(aes(house$Price,fill=house$Type),alpha=0.4)+
        labs(x='price')+theme(legend.title = element_blank())

```


#### 4.2.6 Rooms, bedroom2

Rooms and Bedroom2 are very similar, which is the number of bedrooms in the property. Rooms has no missing value and bedroom2 has 23% percent missing value and is scraped by the contributor from different sources.

```{r}
ggplot(house)+geom_bar(aes(x=house$Rooms), fill='grey',col='black')+
        coord_cartesian(xlim=c(0,16),)+ 
        scale_x_continuous(breaks=seq(0, 16, 1))+
        labs(x='Number of Room')

```        

#### 4.2.7 Car
Car is the number of parking spots each property has. The bar chart shows the distribution. Car has 1.67% missing values. I imputed the missing value using median.


```{r}
house[is.na(house$Car),]$Car=median(house$Car,na.rm=T)
```

```{r}
ggplot(house)+geom_histogram(aes(x=house$Car),
                        col='black',fill='grey')+
        coord_cartesian(xlim=c(0,10),)+
        scale_x_continuous(breaks=seq(0, 10, 1))+
        labs(x='number of parking spot',
             y='number of properties')

```

#### 4.2.8 Distance

Distance is the distance in km from the property to the center business district. I converted its data type to numeric.


```{r}
house$Distance=as.character(house$Distance)
house$Distance=as.numeric(house$Distance)
```

```{r}
ggplot(house)+geom_histogram(aes(x=house$Distance), 
                             fill='grey',col='black',binwidth = 2)+
        labs(x='Distance to CBD in km', y='number of properties')
```

#### 4.2.9 YearBuilt
The year the property is built. I replace missing values and one data built before 1880 with median value. Below is the distribution chart for houses built in different period.


```{r}
house[is.na(house$YearBuilt),]$YearBuilt=median(house$YearBuilt,na.rm=T)
house[house$YearBuilt<1800,]$YearBuilt=1970
```


```{r}
df_year=as.data.frame(table(house$YearBuilt))
colnames(df_year)=c('year','count')
df_year$year=as.numeric(as.character(df_year$year))
df_year$year50 <- cut(df_year$year, c(0, 1899,1949,1979,1999,2050),
                      include.lowest = T,
                      right = TRUE)
levels(df_year$year50) <- c("<1900", "1900~1949", "1950~1979", "1980~1999",'>2000')
ggplot(df_year)+geom_bar(aes(x=df_year$year50, y=df_year$count),
                         stat = "identity",
                         fill='black', width = 0.5)+
        labs(title="Histogram for YearBuilt", x='year',
             y='number of properties')+
        coord_flip()

```

#### 4.2.10 Date & AVGmonth_price

Date is the date the property was sold. The plots show monthly trends of the total number of properties sold and average sold price per square meter. 

```{r}

df_num=data.frame(table(house$Date))
colnames(df_num)=c('Date','count')
df_num$Date=dmy(df_num$Date)
df_num$Date=floor_date(df_num$Date, unit = "month")
df_num=aggregate(df_num$count~df_num$Date,df_num,sum)
df_num=df_num[c(-1,-2),]
pd1= ggplot(df_num)+geom_line(aes(x=df_num$`df_num$Date`,y=df_num$`df_num$count`),
                                   color="grey")+
        geom_point(aes(x=df_num$`df_num$Date`,y=df_num$`df_num$count`),
                                                          color="black")+
        labs(title='Property sold per month',x='date', y='number of properties sold ')


df_avghouseprice=aggregate(house$PriceSQM~house$Date,house,mean)
colnames(df_avghouseprice)=c('Date','average sold price')
df_avghouseprice$Date=dmy(df_avghouseprice$Date)
df_avghouseprice$Date=floor_date(df_avghouseprice$Date, unit = "month")
df_avghouseprice=aggregate(df_avghouseprice$`average sold price`~df_avghouseprice$Date,
                           df_avghouseprice,mean)
df_avghouseprice=df_avghouseprice[-1,]
colnames(df_avghouseprice)=c('Date','average sold price')
pd2= ggplot(df_avghouseprice)+geom_line(aes(x=df_avghouseprice$Date,y=df_avghouseprice$`average sold price`),
                                   color="grey")+
        geom_point(aes(x=df_avghouseprice$Date,y=df_avghouseprice$`average sold price`),
                  color="black")+
        labs(title='Average sold price per square meter',x='date', y='average sold price per square meter')
grid.arrange(pd1, pd2, ncol=2)
```
Instead of using the date as a time feature, we would use monthly price per square meter as a variable which reflects market seasonal fluctuation due to the change in demand and supply. There are only 16 data points for the first month these data points of the first month are removed.

```{r}
house$Date=dmy(house$Date)
house$Date=floor_date(house$Date, unit = "month")
house=house[!(house$Date=='2016-02-01'),]
house$AVGprice=df_avghouseprice[match(house$Date,df_avghouseprice$Date),2]
```


#### 4.2.11 Method

Method reflects how the house was sold. Below are the meanings of different method and the distribution chart.
PI - property passed in; 
S - sold not disclosed; 
SA - sold after auction; 
SP - property sold prior; 
VB - vendor bid
```{r}
house$Method=as.character(house$Method)
house$Method=as.factor(house$Method)
```

```{r}
ggplot(house)+geom_bar(aes(x=house$Method), fill='grey', col='black',
              width = 0.5)+
        theme(axis.text.x = element_text(angle = 30,vjust = 0.5))+
        labs(x='method')

```

### 4.3 Feature selection 

The following feature would be removed because either better features has been engineered so they become redundant or they are irrelevant to the prediction.  

Removed features: Date, SellerG, Suburb, Address, Postcode, Bedroom2, Lattitude, Longtitude, PriceSQM, Price

```{r}
#remove the variables that I don't use
house=house[,-which(names(house)=='Date')]
house=house[,-which(names(house)=='SellerG')]
house=house[,-which(names(house)=='Suburb')]
house=house[,-which(names(house)=='Address')]
house=house[,-which(names(house)=='Postcode')]
house=house[,-which(names(house)=='Bedroom2')]
house=house[,-which(names(house)=='Lattitude')]
house=house[,-which(names(house)=='Longtitude')]
house=house[,-which(names(house)=='PriceSQM')]
house=house[,-which(names(house)=='Price')]
```


```{r}
house_numeric=house
house_numeric=house_numeric[,-which(names(house_numeric)=='Type')]
house_numeric=house_numeric[,-which(names(house_numeric)=='Method')]
house_numeric=house_numeric[,-which(names(house_numeric)=='CouncilArea')]
house_numeric=house_numeric[,-which(names(house_numeric)=='Regionname')]

corr <- cor(house_numeric)
```

```{r}
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlation of numeric variables", 
           ggtheme=theme_bw)

```

We implemented forward, backward, forward stepwise and backward stepwise for the best feature, which all select the same features except for BuildingAreaRatio. We exclude BuildingAreaRatio in our final dataset.

```{r, eval=F, echo=T}
null=lm(log_Price~1,data=house)
full=lm(log_Price~.,data=house)
stepAIC(null, scope=list(lower=null, upper=full), direction= "forward", trace=TRUE)
stepAIC(full, direction= "backward", trace=TRUE)
stepAIC(null, scope=list(lower=null, upper=full), direction= "both", trace=TRUE)
stepAIC(full, direction= "both", trace=TRUE)
```

```{r}
house=house[,-which(names(house)=='BuildingAreaRatio')]
```
The final dataset for modelling has 14 variables, 10038 observations. There is no missing values in the dataset.
```{r}
vis_miss(house)
```


##5. Modelling & Evaluation

### 5.1 linear model

Below is the details of the linear model. Adjusted R squared of 0.8208 with close to 0 p-value. log_Pice is the dependent variable and the rest variables are regressors.

```{r}
model_linear=lm(formula = log_Price ~., data = house)
summary(model_linear)
residual=data.frame(residual=model_linear$residuals, prediction=model_linear$fitted.values)
pred=data.frame(price=model_linear$model$log_Price, prediction=model_linear$fitted.values)
```


```{r}
ggplot(residual)+geom_histogram(aes(x=residual),
                                binwidth = 0.1, fill='grey', col='black')+
        coord_cartesian(xlim=c(-1.5,1.5))+
        scale_x_continuous(breaks=seq(-1.5,1.5,0.5))+
        labs(title = 'histogram of residuals')
```

```{r}
ggplot(pred)+geom_point(aes(y=prediction,x=price), color='blue', alpha=0.10)+
        coord_cartesian(xlim=c(-2,2), ylim=c(-2,2))+
        labs( x='log_Price', y='prediction')
```

```{r}
ggplot(residual)+geom_point(aes(x=prediction,y=residual), color='purple', alpha=0.10)+
        labs(x='prediction', y='residual')+
        coord_cartesian(xlim=c(-2,2), ylim=c(-0.7,0.7))
```

#### Model Evaluation:

I use the package caret for 4-fold cross validation with 10 repeats so I can get a sample of 40 test-set results of RMSE. Below is the Boxplot of these 40 RMSE test-set results with a mean of 0.2189.

```{r}
set.seed(666)
custom <- trainControl(method = "repeatedcv",
                       number = 4,
                       repeats= 10,
                       verboseIter = F)

eva_linear <- train(log_Price ~., house,
            method='lm',
            trControl=custom)
```

```{r}
mev_linear=data.frame(eva_linear$resample)
mev_linear$type='Linear Regression'
mev_linear=mev_linear[,-which(names(mev_linear)=='Rsquared')]

ggplot(mev_linear)+geom_boxplot(aes(mev_linear$type,mev_linear$RMSE,),
                             fill='grey', col='black')+
        labs(x='',y='RMSE')
print(paste0("Mean RMSE of linear model is ", mean(mev_linear$RMSE)))

```

### 5.2 Decision Tree
I appled simple decision tree model to the dataset and tuned the parameter mincriterion, maxdepth to find the combination achieving the best performance.The combination of these two parameter is how the trees are pruned.
I found that when maxdepth is 0 and mincriterion is 0.1, simple decision tree model has the best performance.
Below shows the RMSEs of the trees when maxdepth is 0 and mincriterion varied from 0.1 to 1. We can see that when mincriterion is 0.1, RMSE is the smallest.
![](./map/treeP.png)
Below is the evaluation of tree model and its comparison with the linear model
```{R, cache = TRUE}
eva_tree <- train(log_Price ~., data = house,
                    method='ctree2',
                    tuneGrid=expand.grid(mincriterion =c(0.1),
                                         maxdepth=0),
                    trControl=custom)
```

```{r}
mev_tree = data.frame(eva_tree$resample)
mev_tree$type='Decision Tree'
mev_tree=mev_tree[,-which(names(mev_tree)=='Rsquared')]

ggplot(mev_tree)+geom_histogram(aes(x=mev_tree$RMSE),
                                  binwidth = 0.0025, fill='grey', col='black')

t.test(mev_linear$RMSE,mev_tree$RMSE)
print(paste0("Mean RMSE of linear model is ", mean(mev_linear$RMSE)))
print(paste0("Mean RMSE of tree model is ", mean(mev_tree$RMSE)))
mev_all=rbind(mev_linear,mev_tree)

ggplot(mev_all)+geom_boxplot(aes(mev_all$type,mev_all$RMSE, fill=mev_all$type))+
        theme(legend.title = element_blank())+
        labs(x='',y='RMSE', title = 'Boxplots of cross-validation results of different models ')+
        theme(axis.text.x=element_blank())
```

From the t.test and distribution plot above we can find that there is no signifcant difference between Decision Tree and Linear Regression. 


#### Random Forest


I applied random forest to the model. For parameter tuning, I tested 500 trees with number of variable as 3, 7, 11. After 1 hour of testing running, when the number of variable is 11, model has the minimum RMSE. Below is the result of the parameter tuning.

![](./map/RandomForest.png)

I compared the RMSE distribution of random forest with decision tree and linear regression. We find that random forest signicantly improved the model performance however the model building is much more time consuming.
```{r}
eva_Forest <- train(log_Price ~., data = house,
                  method='rf',
                  tuneGrid=expand.grid(mtry =c(11)),
                  trControl=custom)
```

```{r}
mev_Forest = data.frame(eva_Forest$resample)
mev_Forest$type='Random Forest'
mev_Forest=mev_Forest[,-which(names(mev_Forest)=='Rsquared')]

ggplot(mev_Forest)+geom_histogram(aes(x=mev_Forest$RMSE),
                                binwidth = 0.0025, fill='grey', col='black')

print(paste0("Mean RMSE of Random Forest model is ", mean(mev_Forest$RMSE)))
mev_all=rbind(mev_all,mev_Forest)

ggplot(mev_all)+geom_boxplot(aes(mev_all$type,mev_all$RMSE, fill=mev_all$type))+
        theme(legend.title = element_blank())+
        labs(x='',y='RMSE', title = 'Boxplots of cross-validation results of different models ')+
        theme(axis.text.x=element_blank())
  
```

#### Ridge, Lasso, Elastic Regression

I built the Ridge, Lasso and Elastic Regression regression using 'Caret' package. Tuning the paramter alpha and lambda to find the best parameter combination. When alpha is 0, the model is ridge regression. When alpha is 1, the model is lasso regression. When alpha is between 0 and 1, the model is Elastic regression. Below shows the RMSE for different alpha and lambda combination. We can see that when alpha=0 and lambda=0.1, the model performs best.

![](./map/Elastic1.png)

I want to see if lambda is continuely lowered, the performance could be improved. Below is the result. Keeping alpha 0, lambda= 0.01 is still the best model.

![](./map/Elastic2.png)


```{r}
eva_ElasticNet <- train(log_Price ~., data = house,
                        method='glmnet',
                        tuneGrid=expand.grid(alpha=0,
                                             lambda=0.01),
                        trControl=custom)
```

```{r}
mev_ElasticNet = data.frame(eva_ElasticNet$resample)
mev_ElasticNet$type='Ridge Regression'
mev_ElasticNet=mev_ElasticNet[,-which(names(mev_ElasticNet)=='Rsquared')]

ggplot(mev_ElasticNet)+geom_histogram(aes(x=mev_ElasticNet$RMSE),
                                  binwidth = 0.0025, fill='grey', col='black')

print(paste0("Mean RMSE of Ridge Regression model is ", mean(mev_ElasticNet$RMSE)))
mev_all=rbind(mev_all,mev_ElasticNet)
ggplot(mev_all)+geom_boxplot(aes(mev_all$type,mev_all$RMSE, fill=mev_all$type))+
        theme(legend.title = element_blank())+
        labs(x='',y='RMSE', title = 'Boxplots of cross-validation results of different models ')+
        theme(axis.text.x=element_blank())

```
#### XGBoost

Applied XGBoost to the model. Below is an example while I was tuning the parameters to find the best performance model. We can see more trees produce better result and 0.1 is the best shrinkage. With the increasing number of trees, the model building becomes slower. The package I use is "caret" and "xgboost". The tuning parameter for the final model are nrounds=500, eta=0.1,gamma=0,max_depth=6,colsample_bytree=1,min_child_weight=1,subsample=1.

![](./map/xgbboost.png)

```{r}
eva_XGBoost <- train(log_Price ~., data = house,
                  method='xgbTree',
                  tuneGrid=expand.grid(nrounds=500,
                                       eta=0.1,
                                       gamma=0,
                                       max_depth=6,
                                       colsample_bytree=1,
                                       min_child_weight=1,
                                       subsample=1),
                  trControl=custom)
```

Below we can find that XGBoost have the best performance among all the models.
```{r}
mev_XGBoost = data.frame(eva_XGBoost$resample)
mev_XGBoost$type='XGBoost'
mev_XGBoost=mev_XGBoost[,-which(names(mev_XGBoost)=='Rsquared')]

ggplot(mev_XGBoost)+geom_histogram(aes(x=mev_XGBoost$RMSE),
                                      binwidth = 0.0025, fill='grey', col='black')

print(paste0("Mean RMSE of Ridge Regression model is ", mean(mev_XGBoost$RMSE)))
mev_all=rbind(mev_all,mev_XGBoost)
ggplot(mev_all)+geom_boxplot(aes(mev_all$type,mev_all$RMSE, fill=mev_all$type))+
        theme(legend.title = element_blank())+
        labs(x='',y='RMSE')+
        theme(axis.text.x=element_blank())
```


## Conclusion

70 percent of the time spent on data exploration
log helps. 
tree model slow to build and tuning parameters but better performance. xgboost is faster to build and performed better than random forest.



## Reference

De Jonge, E., & Van Der Loo, M. (2013). An introduction to data cleaning with R. Heerlen: Statistics Netherlands.

Datar, R., & Garg, H. (2019). Hands-on exploratory data analysis with R: Become an expert in exploratory data analysis using R packages (1st ed.). UK: Packt Publishing.

Kahle, D., & Wickham, H. (2013). ggmap: Spatial visualization with ggplot2. The R Journal, 5(1), 144. doi:10.32614/RJ-2013-014

James, G., Hastie, T., Tibshirani, R., & Witten, D. (2013). An introduction to statistical learning: With applications in R Springer New York.

Hastie, T., Tibshirani, R., Friedman, J. H., & SpringerLink (Online service). (2009;2013;2001;). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). New York, NY: Springer. doi:10.1007/978-0-387-21606-5

Pino, T. P. (2018, October 14). Melbourne Housing Market. Retrieved May 7, 2020, from https://www.kaggle.com/anthonypino/melbourne-housing-marke

Melbourne Suburb Map. (n.d.). Retrieved June 7, 2020, from http://melbournesuburbsmap.com

LocalCouncilMaps. (n.d.). Retrieved June 7, 2020, from https://enrol.vec.vic.gov.au/ElectoralBoundaries/LocalCouncilMaps.html

Melbourne Regions. (n.d.). Retrieved June 7, 2020, from https://www.education.vic.gov.au/about/department/structure/Pages/regions.aspx



